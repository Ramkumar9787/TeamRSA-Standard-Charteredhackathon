{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkbVWvOecUOq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.cm as cm\n",
        "from scipy import ndimage\n",
        "from skimage.measure import regionprops\n",
        "from skimage import io\n",
        "from skimage.filters import threshold_otsu   # For finding the threshold for grayscale to binary conversion\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from time import time\n",
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fofoBwfYeI8L"
      },
      "outputs": [],
      "source": [
        "genuine_image_paths = '/content/drive/MyDrive/datasets/hackathon/real'\n",
        "forged_image_paths = '/content/drive/MyDrive/datasets/hackathon/forged'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFmSOgBveLRx"
      },
      "outputs": [],
      "source": [
        "def rgbgrey(img):\n",
        "    # Converts rgb to grayscale\n",
        "    greyimg = np.zeros((img.shape[0], img.shape[1]))\n",
        "    for row in range(len(img)):\n",
        "        for col in range(len(img[row])):\n",
        "            greyimg[row][col] = np.average(img[row][col])\n",
        "    return greyimg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuW9MlrXeOMW"
      },
      "outputs": [],
      "source": [
        "def greybin(img):\n",
        "    # Converts grayscale to binary\n",
        "    blur_radius = 0.8\n",
        "    img = ndimage.gaussian_filter(img, blur_radius)  # to remove small components or noise\n",
        "#     img = ndimage.binary_erosion(img).astype(img.dtype)\n",
        "    thres = threshold_otsu(img)\n",
        "    binimg = img > thres\n",
        "    binimg = np.logical_not(binimg)\n",
        "    return binimg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyCl0t_zeRnN"
      },
      "outputs": [],
      "source": [
        "def preproc(path, img=None, display=True):\n",
        "    if img is None:\n",
        "        img = mpimg.imread(path)\n",
        "    if display:\n",
        "        plt.imshow(img)\n",
        "        plt.show()\n",
        "    grey = rgbgrey(img) #rgb to grey\n",
        "    if display:\n",
        "        plt.imshow(grey, cmap = matplotlib.cm.Greys_r)\n",
        "        plt.show()\n",
        "    binimg = greybin(grey) #grey to binary\n",
        "    if display:\n",
        "        plt.imshow(binimg, cmap = matplotlib.cm.Greys_r)\n",
        "        plt.show()\n",
        "    r, c = np.where(binimg==1)\n",
        "    # Now we will make a bounding box with the boundary as the position of pixels on extreme.\n",
        "    # Thus we will get a cropped image with only the signature part.\n",
        "    signimg = binimg[r.min(): r.max(), c.min(): c.max()]\n",
        "    if display:\n",
        "        plt.imshow(signimg, cmap = matplotlib.cm.Greys_r)\n",
        "        plt.show()\n",
        "    return signimg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrMCqF21eShY"
      },
      "outputs": [],
      "source": [
        "def Ratio(img):\n",
        "    a = 0\n",
        "    for row in range(len(img)):\n",
        "        for col in range(len(img[0])):\n",
        "            if img[row][col]==True:\n",
        "                a = a+1\n",
        "    total = img.shape[0] * img.shape[1]\n",
        "    return a/total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5AaoA0VeZm-"
      },
      "outputs": [],
      "source": [
        "def Centroid(img):\n",
        "    numOfWhites = 0\n",
        "    a = np.array([0,0])\n",
        "    for row in range(len(img)):\n",
        "        for col in range(len(img[0])):\n",
        "            if img[row][col]==True:\n",
        "                b = np.array([row,col])\n",
        "                a = np.add(a,b)\n",
        "                numOfWhites += 1\n",
        "    rowcols = np.array([img.shape[0], img.shape[1]])\n",
        "    centroid = a/numOfWhites\n",
        "    centroid = centroid/rowcols\n",
        "    return centroid[0], centroid[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuvszfDfebse"
      },
      "outputs": [],
      "source": [
        "def EccentricitySolidity(img):\n",
        "    r = regionprops(img.astype(\"int8\"))\n",
        "    return r[0].eccentricity, r[0].solidity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n4PAd-zedUs"
      },
      "outputs": [],
      "source": [
        "def SkewKurtosis(img):\n",
        "    h,w = img.shape\n",
        "    x = range(w)  # cols value\n",
        "    y = range(h)  # rows value\n",
        "    #calculate projections along the x and y axes\n",
        "    xp = np.sum(img,axis=0)\n",
        "    yp = np.sum(img,axis=1)\n",
        "    #centroid\n",
        "    cx = np.sum(x*xp)/np.sum(xp)\n",
        "    cy = np.sum(y*yp)/np.sum(yp)\n",
        "    #standard deviation\n",
        "    x2 = (x-cx)**2\n",
        "    y2 = (y-cy)**2\n",
        "    sx = np.sqrt(np.sum(x2*xp)/np.sum(img))\n",
        "    sy = np.sqrt(np.sum(y2*yp)/np.sum(img))\n",
        "\n",
        "    #skewness\n",
        "    x3 = (x-cx)**3\n",
        "    y3 = (y-cy)**3\n",
        "    skewx = np.sum(xp*x3)/(np.sum(img) * sx**3)\n",
        "    skewy = np.sum(yp*y3)/(np.sum(img) * sy**3)\n",
        "\n",
        "    #Kurtosis\n",
        "    x4 = (x-cx)**4\n",
        "    y4 = (y-cy)**4\n",
        "    # 3 is subtracted to calculate relative to the normal distribution\n",
        "    kurtx = np.sum(xp*x4)/(np.sum(img) * sx**4) - 3\n",
        "    kurty = np.sum(yp*y4)/(np.sum(img) * sy**4) - 3\n",
        "\n",
        "    return (skewx , skewy), (kurtx, kurty)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZhzBLkYefmH"
      },
      "outputs": [],
      "source": [
        "def getFeatures(path, img=None, display=False):\n",
        "    if img is None:\n",
        "        img = mpimg.imread(path)\n",
        "    img = preproc(path, display=display)\n",
        "    ratio = Ratio(img)\n",
        "    centroid = Centroid(img)\n",
        "    eccentricity, solidity = EccentricitySolidity(img)\n",
        "    skewness, kurtosis = SkewKurtosis(img)\n",
        "    retVal = (ratio, centroid, eccentricity, solidity, skewness, kurtosis)\n",
        "    return retVal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuU2NzP1eiGU"
      },
      "outputs": [],
      "source": [
        "def getCSVFeatures(path, img=None, display=False):\n",
        "    if img is None:\n",
        "        img = mpimg.imread(path)\n",
        "    temp = getFeatures(path, display=display)\n",
        "    features = (temp[0], temp[1][0], temp[1][1], temp[2], temp[3], temp[4][0], temp[4][1], temp[5][0], temp[5][1])\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cMljq9hej8B"
      },
      "outputs": [],
      "source": [
        "def makeCSV():\n",
        "    if not(os.path.exists('/content/drive/MyDrive/datasets/hackathon/Features')):\n",
        "        os.mkdir('/content/drive/MyDrive/datasets/hackathon/Features')\n",
        "        print('New folder \"Features\" created')\n",
        "    if not(os.path.exists('/content/drive/MyDrive/datasets/hackathon/Features/Training')):\n",
        "        os.mkdir('/content/drive/MyDrive/datasets/hackathon/Features/Training')\n",
        "        print('New folder \"Features/Training\" created')\n",
        "    if not(os.path.exists('/content/drive/MyDrive/datasets/hackathon/Features/Testing')):\n",
        "        os.mkdir('/content/drive/MyDrive/datasets/hackathon/Features/Testing')\n",
        "        print('New folder \"Features/Testing\" created')\n",
        "    # genuine signatures path\n",
        "    gpath = genuine_image_paths\n",
        "    # forged signatures path\n",
        "    fpath = forged_image_paths\n",
        "    for person in range(1,13):\n",
        "        per = ('00'+str(person))[-3:]\n",
        "        print('Saving features for person id-',per)\n",
        "\n",
        "        with open('/content/drive/MyDrive/datasets/hackathon/Features/Training/training_'+per+'.csv', 'w') as handle:\n",
        "            handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y,output\\n')\n",
        "            # Training set\n",
        "            for i in range(0,3):\n",
        "                source = os.path.join(gpath, per+per+'_00'+str(i)+'.png')\n",
        "                features = getCSVFeatures(path=source)\n",
        "                handle.write(','.join(map(str, features))+',1\\n')\n",
        "            for i in range(0,3):\n",
        "                source = os.path.join(fpath, '021'+per+'_00'+str(i)+'.png')\n",
        "                features = getCSVFeatures(path=source)\n",
        "                handle.write(','.join(map(str, features))+',0\\n')\n",
        "\n",
        "        with open('/content/drive/MyDrive/datasets/hackathon/Features/Testing/testing_'+per+'.csv', 'w') as handle:\n",
        "            handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y,output\\n')\n",
        "            # Testing set\n",
        "            for i in range(3, 5):\n",
        "                source = os.path.join(gpath, per+per+'_00'+str(i)+'.png')\n",
        "                features = getCSVFeatures(path=source)\n",
        "                handle.write(','.join(map(str, features))+',1\\n')\n",
        "            for i in range(3,5):\n",
        "                source = os.path.join(fpath, '021'+per+'_00'+str(i)+'.png')\n",
        "                features = getCSVFeatures(path=source)\n",
        "                handle.write(','.join(map(str, features))+',0\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFKWgLv5emIT",
        "outputId": "2e951510-517d-4bba-a5b3-e21f6dd35653"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving features for person id- 001\n",
            "Saving features for person id- 002\n",
            "Saving features for person id- 003\n",
            "Saving features for person id- 004\n",
            "Saving features for person id- 005\n",
            "Saving features for person id- 006\n",
            "Saving features for person id- 007\n",
            "Saving features for person id- 008\n",
            "Saving features for person id- 009\n",
            "Saving features for person id- 010\n",
            "Saving features for person id- 011\n",
            "Saving features for person id- 012\n"
          ]
        }
      ],
      "source": [
        "makeCSV()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0IbyXaLeoFD"
      },
      "outputs": [],
      "source": [
        "def testing(path):\n",
        "    feature = getCSVFeatures(path)\n",
        "    if not(os.path.exists('/content/drive/MyDrive/datasets/hackathon/TestFeatures')):\n",
        "        os.mkdir('/content/drive/MyDrive/datasets/hackathon/TestFeatures')\n",
        "    with open('/content/drive/MyDrive/datasets/hackathon/TestFeatures/testcsv.csv', 'w') as handle:\n",
        "        handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y\\n')\n",
        "        handle.write(','.join(map(str, feature))+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "RAm70UwXep6A",
        "outputId": "b26ef6d8-d0c4-46b6-82bd-19a1e5dd5c9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter person's id : 003\n",
            "Enter path of signature image : /content/drive/MyDrive/datasets/hackathon/real/003003_000.png\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 4, got 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-e57e097ecaa6>\u001b[0m in \u001b[0;36m<cell line: 110>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-e57e097ecaa6>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(train_path, test_path, type2)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorr_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorr_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadCSV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorr_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "n_input = 9\n",
        "train_person_id = input(\"Enter person's id : \")\n",
        "test_image_path = input(\"Enter path of signature image : \")\n",
        "train_path = '/content/drive/MyDrive/datasets/hackathon/Features/Training/training_'+train_person_id+'.csv'\n",
        "# Replace 'F:\\\\Axis dataset\\\\Dataset\\\\TestFeatures/testcsv.csv' with your actual path\n",
        "test_path = '/content/drive/MyDrive/datasets/hackathon/TestFeatures/testcsv.csv'\n",
        "\n",
        "def readCSV(train_path, test_path, type2=False):\n",
        "    # Reading train data\n",
        "    df = pd.read_csv(train_path, usecols=range(n_input))\n",
        "    train_input = np.array(df.values)\n",
        "    train_input = train_input.astype(np.float32, copy=False)  # Converting input to float_32\n",
        "    df = pd.read_csv(train_path, usecols=(n_input,))\n",
        "    temp = [elem[0] for elem in df.values]\n",
        "    correct = np.array(temp)\n",
        "    corr_train = tf.keras.utils.to_categorical(correct, 2)      # Converting to one hot\n",
        "    # Reading test data\n",
        "    df = pd.read_csv(test_path, usecols=range(n_input))\n",
        "    test_input = np.array(df.values)\n",
        "    test_input = test_input.astype(np.float32, copy=False)\n",
        "    if not(type2):\n",
        "        df = pd.read_csv(test_path, usecols=(n_input,))\n",
        "        temp = [elem[0] for elem in df.values]\n",
        "        correct = np.array(temp)\n",
        "        corr_test = tf.keras.utils.to_categorical(correct, 2)      # Converting to one hot\n",
        "    if not(type2):\n",
        "        return train_input, corr_train, test_input, corr_test\n",
        "    else:\n",
        "        return train_input, corr_train, test_input\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 1000\n",
        "display_step = 1\n",
        "\n",
        "# Network Parameters\n",
        "n_hidden_1 = 7 # 1st layer number of neurons\n",
        "n_hidden_2 = 10 # 2nd layer number of neurons\n",
        "n_hidden_3 = 30 # 3rd layer\n",
        "n_classes = 2 # no. of classes (genuine or forged)\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    'h1': tf.Variable(tf.random.normal([n_input, n_hidden_1], seed=1)),\n",
        "    'h2': tf.Variable(tf.random.normal([n_hidden_1, n_hidden_2])),\n",
        "    'h3': tf.Variable(tf.random.normal([n_hidden_2, n_hidden_3])),\n",
        "    'out': tf.Variable(tf.random.normal([n_hidden_3, n_classes], seed=2))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random.normal([n_hidden_1], seed=3)),\n",
        "    'b2': tf.Variable(tf.random.normal([n_hidden_2])),\n",
        "    'b3': tf.Variable(tf.random.normal([n_hidden_3])),\n",
        "    'out': tf.Variable(tf.random.normal([n_classes], seed=4))\n",
        "}\n",
        "\n",
        "# Create model\n",
        "def multilayer_perceptron(x):\n",
        "    layer_1 = tf.tanh((tf.matmul(x, weights['h1']) + biases['b1']))\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
        "    out_layer = tf.tanh(tf.matmul(layer_3, weights['out']) + biases['out'])\n",
        "    return out_layer\n",
        "\n",
        "# Define loss and optimizer\n",
        "def loss_op(logits, labels):\n",
        "    return tf.reduce_mean(tf.square(logits - labels))\n",
        "\n",
        "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# For accuracies\n",
        "def accuracy(y_pred, y_true):\n",
        "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
        "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = multilayer_perceptron(x)\n",
        "        loss = loss_op(logits, y)\n",
        "    gradients = tape.gradient(loss, list(weights.values()) + list(biases.values()))\n",
        "    optimizer.apply_gradients(zip(gradients, list(weights.values()) + list(biases.values())))\n",
        "\n",
        "def evaluate(train_path, test_path, type2=False):\n",
        "    train_input, corr_train, test_input, corr_test = readCSV(train_path, test_path, type2)\n",
        "    for epoch in range(training_epochs):\n",
        "        train_step(train_input, corr_train)\n",
        "        cost = loss_op(multilayer_perceptron(train_input), corr_train).numpy()\n",
        "        if cost < 0.0001:\n",
        "            break\n",
        "    accuracy1 = accuracy(multilayer_perceptron(train_input), corr_train).numpy()\n",
        "    if type2 is False:\n",
        "        accuracy2 = accuracy(multilayer_perceptron(test_input), corr_test).numpy()\n",
        "        return accuracy1, accuracy2\n",
        "    else:\n",
        "        prediction = multilayer_perceptron(test_input)\n",
        "        if prediction[0][1] > prediction[0][0]:\n",
        "            print('Genuine Image')\n",
        "            return True\n",
        "        else:\n",
        "            print('Forged Image')\n",
        "            return False\n",
        "\n",
        "evaluate(train_path, test_path, type2=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}